from huggingface_hub import InferenceClient
import re
import os
import json

# ANSI color codes for console output
GREEN = "\033[92m"
YELLOW = "\033[93m"
RED = "\033[91m"
RESET = "\033[0m"

# Primary and fallback models
PRIMARY_MODEL = "meta-llama/Llama-3.2-1B"   # gated, requires Meta approval
FALLBACK_MODEL = "google/gemma-7b-it"
  # open, works immediately

# Initialize Hugging Face client with your token (set HF_TOKEN in environment)
client = InferenceClient(
    PRIMARY_MODEL,
    token=os.getenv("HF_TOKEN")
)

USE_REMOTE = True
token_usage = {"input": 0, "output": 0, "total": 0}  # usage counter


def plan_workflow(user_input: str) -> str:
    if USE_REMOTE:
        # Define prompt once, available for both primary and fallback
        prompt = f"""
        You are an expert orchestrator.
        Convert the following request into a structured workflow plan.
        Request: {user_input}
        Available APIs: flight_api, hotel_api, weather_api, activity_api, stock_api
        Output ONLY valid JSON with correct parameters from the request.
        """

        try:
            # Count input tokens (approx: 1 token ≈ 4 chars)
            token_usage["input"] += len(prompt) // 4

            response = client.text_generation(
                prompt,
                max_new_tokens=300,
                temperature=0.2
            )

            # Count output tokens (approx: 1 token ≈ 4 chars)
            token_usage["output"] += len(response) // 4
            token_usage["total"] = token_usage["input"] + token_usage["output"]

            print(f"{GREEN}[INFO] Workflow generated by {PRIMARY_MODEL}{RESET}")
            print(f"{GREEN}[USAGE] Input: {token_usage['input']} | Output: {token_usage['output']} | Total: {token_usage['total']} tokens{RESET}")
            print(f"{GREEN}[SUMMARY] Final workflow source: LLaMA-3{RESET}")
            return response

        except Exception as e:
            print(f"{YELLOW}[WARN] Remote {PRIMARY_MODEL} failed: {e}{RESET}")
            print(f"{YELLOW}[INFO] Switching to fallback model: {FALLBACK_MODEL}{RESET}")

            # Switch to Mistral-7B (chat_completion API)
            fallback_client = InferenceClient(
                FALLBACK_MODEL,
                token=os.getenv("HF_TOKEN")
            )
            try:
                response = fallback_client.text_generation(
                prompt, max_new_tokens=300, temperature=0.2 
                )
                output = response
                print(f"{GREEN}[INFO] Workflow generated by {FALLBACK_MODEL}{RESET}")
                print(f"{GREEN}[SUMMARY] Final workflow source: Mistral-7B{RESET}")
                return output
            except Exception as e2:
                print(f"{RED}[ERROR] Remote fallback model failed: {e2}{RESET}")
                print(f"{YELLOW}[INFO] Workflow generated by local regex fallback{RESET}")
                print(f"{YELLOW}[SUMMARY] Final workflow source: Regex Parser{RESET}")
                return local_fallback(user_input)
    else:
        print(f"{YELLOW}[INFO] Workflow generated by local regex fallback{RESET}")
        print(f"{YELLOW}[SUMMARY] Final workflow source: Regex Parser{RESET}")
        return local_fallback(user_input)


def local_fallback(user_input: str) -> str:
    """
    Simple regex-based fallback parser.
    Extracts origin, destination, and activity keywords from user input.
    """
    origin_match = re.search(r"from\s+(\w+)", user_input, re.IGNORECASE)
    dest_match = re.search(r"to\s+(\w+)", user_input, re.IGNORECASE)
    activity_match = re.search(r"(sports|hiking|skiing|activities?)", user_input, re.IGNORECASE)

    origin = origin_match.group(1) if origin_match else "Unknown"
    destination = dest_match.group(1) if dest_match else "Unknown"
    activity = activity_match.group(1) if activity_match else "general"

    workflow = {
        "workflow": [
            {"api": "flight_api", "params": {"from": origin, "to": destination}},
            {"api": "hotel_api", "params": {"location": destination}},
            {"api": "weather_api", "params": {"location": destination}},
            {"api": "activity_api", "params": {"type": activity, "location": destination}}
        ]
    }
    return json.dumps(workflow)


def extract_json(text: str) -> str:
    match = re.search(r"\{.*\}", text, re.DOTALL)
    if not match:
        raise ValueError("No JSON found in model output")
    return match.group(0)
